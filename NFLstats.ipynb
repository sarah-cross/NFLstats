{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2fab78e",
   "metadata": {},
   "source": [
    "# Analyze NFL Stats with Python\n",
    "\n",
    "### Try some of these resources for extra help as you work:\n",
    "\n",
    "* [View the Analyze NFL Stats with Python cheatsheet](https://www.codecademy.com/learn/case-study-analyze-nfl-stats/modules/nfl-stats-case-study/cheatsheet)\n",
    "* [View the solution notebook](./solution.html)\n",
    "* [Learn more about analyzing NFL stats in this introductory article](https://www.codecademy.com/courses/case-study-analyze-nfl-stats/articles/analyze-nfl-stats-with-python-article)\n",
    "\n",
    "### Looking for a challenge? Try this project without any provided code.\n",
    "* [View the unguided notebook](./unguided.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a1426",
   "metadata": {},
   "source": [
    "## Setup and inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc870b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinear_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LogisticRegression\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388f14f5",
   "metadata": {},
   "source": [
    "### 1. Load the dataset\n",
    "\n",
    "After running the first cell to load all necessary libraries, we need to load our dataset. Using pandas, load the dataset `season_2021.csv` and save it as `nfl`. Inspect the first few rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8667b893",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary style=\"display:list-item;\"><b>Toggle for an overview of the variables in our dataset.</b></summary>\n",
    "\n",
    "* `symbol`: team name abbreviation\n",
    "* `team_name`: team name\n",
    "* `result`: whether this team won (`W`), lost (`L`), or tied (`T`) for this game\n",
    "* `1stD_offense`: First down conversions by the team's offense\n",
    "* `TotYd_offense`: Total yards gained by the team's offense\n",
    "* `PassY_offense`: Total passing yards gained by the team's offense\n",
    "* `RushY_offense`: Total rushing yards gained by the team's offense\n",
    "* `TO_offense`: Turnovers committed by the team's offense\n",
    "* `1stD_defense`: First down conversions allowed by the team's defense\n",
    "* `TotYd_defense`: Total yards allowed by the team's defense\n",
    "* `PassY_defense`: Total passing yards allowed by the team's defense\n",
    "* `RushY_defense`: Total rushing yards allowed by the team's defense\n",
    "* `TO_defense`: Turnovers in favor of the defensive team\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "nfl = pd.read_csv('season_2021.csv')\n",
    "\n",
    "# inspect first few rows\n",
    "print(nfl.head(5))\n",
    "print(nfl.info(show_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838104c7",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary style=\"display:list-item; font-size:16px; color:blue;\"><i>What did we discover in this step? Toggle to check!</i></summary>\n",
    "\n",
    "The NFL dataset consists of comprehensive data on the games that took place throughout the 2021 season. We can see game details along the column axis and each game along the row axis. We can find the name of each team, the date and time of the game, the outcome of the game, and the stats accumulated during the game. \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b022de9",
   "metadata": {},
   "source": [
    "### 2. Summarize outcomes\n",
    "\n",
    "Next, we want to examine our outcome variable to find out how wins and losses are recorded. Check the counts of each value of the `result` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc61ee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result value counts\n",
    "nfl['result'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e85067",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary style=\"display:list-item; font-size:16px; color:blue;\"><i>What did we discover in this step? Toggle to check!</i></summary>\n",
    "\n",
    "The `result` variable is encoded with letters for a win (`W`), a loss (`L`), or a tie (`T`). There were 285 games played, but only 284 with a winner. One of the games was a tie (reported as `T` for each of the two teams who played in that game).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e74299",
   "metadata": {},
   "source": [
    "### 3. Encode the result labels\n",
    "\n",
    "We have two problems with the `result` variable:\n",
    "* The `T` group is very small, which can lead to issues with our model's performance.\n",
    "* Our regression algorithm requires numeric values as the outcome, but ours is coded with letters.\n",
    "\n",
    "We can solve both of these issues in one step! We'll group the tie with the losses and convert to 1 for wins and 0 for ties and losses.\n",
    "\n",
    "Using the provided encoder, use the `.replace()` function to convert the `result` column values to numeric values. Then check the value counts again to make sure you have only two categories that are numbers rather than letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77178bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested dictionary to encode alphanumeric values to numeric values\n",
    "result_encoder = {'result': {'W': 1, 'T': 0, 'L': 0}}\n",
    "\n",
    "# encode result column using encoder\n",
    "nfl.replace(result_encoder, inplace=True)\n",
    "\n",
    "# check result value counts\n",
    "nfl['result'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a23cd9",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary style=\"display:list-item; font-size:16px; color:blue;\"><i>What did we discover in this step? Toggle to check!</i></summary>\n",
    "\n",
    "We combined ties with losses and encoded the group as 0s. Wins were encoded as 1s. We can see we now have two groups that are nearly the same size.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2248c64d",
   "metadata": {},
   "source": [
    "### 4. Visualize the stats\n",
    "\n",
    "Now let's take a moment to explore trends in the stats we will be using to predict wins. The variable `stat` has been set to `1stD_offense` by default. \n",
    "\n",
    "Use `sns.boxplot()` to create a box plot of `stat` by wins and losses. Set the `x`, `y`, and `data` parameters inside the function and save the plot as `stat_plot`.\n",
    "\n",
    "We've included code for plot labels and to view a list of the names of the stats in the dataset. Try changing the value of the `stat` variable to any one of the stat names and run the cell again to see a plot of how losing teams' stats compare to winning teams' stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1c6dc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change stat to view plot\n",
    "stat = 'RushY_offense'\n",
    "\n",
    "# box plot of stat\n",
    "stat_plot = sns.boxplot(data=nfl, x='result', y=stat)\n",
    "\n",
    "# plot labels\n",
    "stat_plot.set_xticklabels(['loss/tie','win'])\n",
    "plt.show()\n",
    "# list feature names\n",
    "print(nfl.columns[8:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1840ce02",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary style=\"display:list-item; font-size:16px; color:blue;\"><i>What did we discover in this step? Toggle to check!</i></summary>\n",
    "\n",
    "Box plots show us the distribution of a stat. For `1stD_offense`, we see that:\n",
    "* First down conversions by the offense are typically between 12 and 33 in winning games (as depicted by the T-shaped ends of the plot). \n",
    "* The middle 50% of winning games appears to cover about 20 to 26 first down conversions (as depicted by the orange box).\n",
    "* The middle line indicates a median of about 23 first down conversions by the winning team. \n",
    "\n",
    "What does this plot tell us when we compare it to first downs in losing games? While there is a range for either, the winning team typically has a higher number of first downs in a game.\n",
    "    \n",
    "The trend we find when looking through all the stats is that winning teams have higher offensive stats on average (indicating more opportunities to score points) and lower defensive stats on average (indicating fewer opportunities for the opponent to score points). This is good news for our machine learning algorithm, as it should be straightforward for the algorithm to learn this pattern among the data.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b608d90",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0669cf1",
   "metadata": {},
   "source": [
    "### 5. Standardize features\n",
    "\n",
    "Before running our regression, we need to prepare our data by standardizing all the game stats. The provided code saves the game stats to a variable named `features` and saves the necessary scaling function as `scaler`.\n",
    "\n",
    "Use the function `scaler.fit()` to fit `features` to the the scaling function. Then use `scaler.transform()` to standardize the game stats. Save this output as `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686fd4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select feature variables to be scaled\n",
    "features = nfl.iloc[:,8:]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the transformer to the features\n",
    "scaler.fit(features)\n",
    "\n",
    "# transform and save as X\n",
    "X = scaler.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2544e02a",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary style=\"display:list-item; font-size:16px; color:blue;\"><i>What did we discover in this step? Toggle to check!</i></summary>\n",
    "\n",
    "How did the functions from the `sklearn` library standardize our stats? The functions transformed our stats by subtracting the mean and dividing by the standard deviation. The result is that each stat now has a mean of 0 and a standard deviation of 1. Some benefits of standardizing include:\n",
    "* All the stats will be put in the same units, so we can compare them to one another and see which were most important to the model later in the process.\n",
    "* Many tuning techniques require standardization. We can use those techniques to improve prediction model accuracy.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a0a2c",
   "metadata": {},
   "source": [
    "### 6. Save game outcomes\n",
    "\n",
    "Let's also separate our game outcome variable for easier reference. Save the game outcomes as a variable called `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff4a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result variable as y\n",
    "y = nfl['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d66b8f",
   "metadata": {},
   "source": [
    "### 7. Split into training and testing data\n",
    "\n",
    "We need to randomly split the data into two groups:\n",
    "* **training data:** we'll use this data to train our model to recognize winning games from patterns in the game stats.\n",
    "* **testing data:** we'll use this data to check our model's accuracy.\n",
    "\n",
    "Use the `train_test_split()` function imported from the `sklearn` library to split the data. This function will split up our features and result labels into training data and testing data, with `test_size` corresponding to the proportion of data reserved for testing. Set `test_size` to 0.5 and `random_state` to 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ebdb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train-test split of the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b92360",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary style=\"display:list-item; font-size:16px; color:blue;\"><i>What did we discover in this step? Toggle to check!</i></summary>\n",
    "\n",
    "We saved our training data as `X_train` (game stats) and `y_train` (game outcome) and our testing data as `X_test` (game stats) and `y_test` (game outcome).\n",
    "    \n",
    "One benefit of using the `train_test_split()` is that rows are selected at random throughout the dataset. This is important in-context because, had we not selected at random, we might bias our model to specific teams or to the early games of the season.\n",
    "    \n",
    "In this case, we are using a test size of 0.5, meaning half of our data will be used to train the model and half will be used to test the model's accuracy. We give `random_state` a number just to guarantee that anyone who runs this notebook will get the same random split that we did.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c21f67",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4bd4ec",
   "metadata": {},
   "source": [
    "### 8. Run the model\n",
    "\n",
    "In this step, we'll train our model to use the patterns of the offensive and defensive stats to predict the probability of a winning game.\n",
    "\n",
    "Create a `LogisticRegression()` classifier and save it to the variable `lrc`. Then call the `.fit()` function using the training data `X_train` and `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the classifier\n",
    "lrc = LogisticRegression()\n",
    "\n",
    "# fit classifier to the training data\n",
    "lrc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211af520",
   "metadata": {},
   "source": [
    "### 9. Check model accuracy\n",
    "\n",
    "With our classifier fitted (trained) to the training data, we can use the trained classifier to make predictions on the test data. Pass the test features `X_test` as a parameter  of `lrc.predict()` and save the resulting predictions as `y_pred`.\n",
    "\n",
    "Now we can check the percentage of outcomes that our model predicted correctly. Use the `accuracy_score()` function imported from the `sklearn` library to compare our predicted test values `y_pred` to the true values `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e1383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with the classifier using the .predict() function\n",
    "y_pred = lrc.predict(X_test)\n",
    "\n",
    "# view the model accuracy with the accuracy_score() function\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce0f99d",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary style=\"display:list-item; font-size:16px; color:blue;\"><i>What did we discover in this step? Toggle to check!</i></summary>\n",
    "\n",
    "We can see from the model performance that we can predict wins and losses with good accuracy. Our model correctly predicted the game outcome for 82.8% of the games in the test set. The next steps might be to try to tune the model to optimize predictive performance.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f3ac6",
   "metadata": {},
   "source": [
    "### 10. Optimize by tuning hyperparameters\n",
    "\n",
    "We can improve our model performance by closely studying how different paremeters affect performance. Let's consider two hyperparameters for the `LogisticRegression` classifer: `penalty` and `C`.\n",
    "\n",
    "* `penalty` imposes a regularization penalty on the model for having too many variables. Our options generally are `l1` and `l2` regularization.\n",
    "* `C` is the inverse of regularization strength. It is applying a penalty to increasing the magnitude of parameter values in order to reduce overfitting.\n",
    "\n",
    "The following code runs a logistic regression on our same data and gets an accuracy score for each combination of `penalty` and `C`. Run the code to see how model accuracy changes when we use different values of these hyperparameters. If you'd like, try changing the values of `C` in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ab861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of penalties\n",
    "penalties = ['l1', 'l2']\n",
    "# create a list of values for C\n",
    "C = [0.01, 0.1, 1.0, 10.0, 1000.0]\n",
    "\n",
    "for penalty in penalties:\n",
    "    for c in C:\n",
    "\n",
    "        # instantiate the classifier\n",
    "        lrc_tuned = LogisticRegression(penalty=penalty, C=c, solver='liblinear')\n",
    "\n",
    "        # fit the classifier to the training data\n",
    "        lrc_tuned.fit(X_train, y_train)\n",
    "        \n",
    "        # predict with the classifier using the .predict() function\n",
    "        y_pred = lrc_tuned.predict(X_test)\n",
    "\n",
    "        # view the model accuracy with the accuracy_score() function\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracy_rd = round(accuracy*100,1)\n",
    "        \n",
    "        # print accuracy for each combination of penalty and C\n",
    "        print(f'Accuracy: {accuracy_rd}% | penalty = {penalty}, C = {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294195cd",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary style=\"display:list-item; font-size:16px; color:blue;\"><i>What did we discover in this step? Toggle to check!</i></summary>\n",
    "\n",
    "A lot of these accuracy scores are very similar (or identical) to our original accuracy score. This is due in part to the fact that `sklearn` automatically uses regularization with `penalty = l2` and `C = 1.0`. While this is not always the case, we gain a small benefit by changing the hyperparameters to `penalty = l1` and `C = 0.1`. This brings us from 82.8% to 84.6% accuracy.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7157c02",
   "metadata": {},
   "source": [
    "### 11. Optimize by changing test size\n",
    "\n",
    "In the cell above, we see that sweeping our parameters did not yield much improvement in prediction accuracy. Let's try another method of parameter tuning: changing the test size of the train-test split. A list of test sizes between 0 and 1 has been coded for you. Similar to the last task, at each test size the code performs a train-test split, fits the model, and computes an accuracy score.\n",
    "\n",
    "Run the code to see how test size affects accuracy. If you'd like, try changing the list of test sizes to get better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59988a50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# optimal penalty and C\n",
    "penalty = 'l1'\n",
    "C = 0.1\n",
    "\n",
    "# create a list of test_sizes\n",
    "test_sizes = [val/100 for val in range(20,36)]\n",
    "\n",
    "for test_size in test_sizes:\n",
    "\n",
    "    # train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    # instantiate the classifier\n",
    "    lrc_tts = LogisticRegression(penalty = penalty, C = C, solver='liblinear')\n",
    "\n",
    "    # fit the classifier to the training data\n",
    "    lrc_tts.fit(X_train, y_train)\n",
    "\n",
    "    # predict with the classifier using the .predict() function\n",
    "    y_pred = lrc_tts.predict(X_test)\n",
    "\n",
    "    # view the model accuracy with the accuracy_score() function\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_rd = round(accuracy*100,1)\n",
    "    \n",
    "    # print accuracy for each combination of penalty and test size\n",
    "    print(f'Accuracy: {accuracy_rd}% | test size = {test_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1fc6d9",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary style=\"display:list-item; font-size:16px; color:blue;\"><i>What did we discover in this step? Toggle to check!</i></summary>\n",
    "\n",
    "As we can see from the output, we were able to improve accuracy slightly with a test size of `0.25`. In this step, we improved from 84.6% correct predictions to 88.8% correct predictions. Nice!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb1c87a",
   "metadata": {},
   "source": [
    "### 12. Save the optimized model\n",
    "\n",
    "Now that we know which parameters optimize our model, let's run and save the final model with our choices for `test_size`, `penalty`, and `C`. Fill in the code to run and save the final model as `optLr`. Continue setting `random_state=42` for the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5de054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the test size and hyperparameters\n",
    "test_size = 0.25\n",
    "penalty = 'l1'\n",
    "C = 0.1\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "# instantiate the classifier\n",
    "optLr = LogisticRegression(penalty = penalty, C = C, solver='liblinear')\n",
    "\n",
    "# fit the classifier to the training data\n",
    "lrc_tts.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d42ccd",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary style=\"display:list-item; font-size:16px; color:blue;\"><i>What did we discover in this step? Toggle to check!</i></summary>\n",
    "\n",
    "We are using `test_size = 0.25`, `penalty = 'l1'`, and `C = 0.1` as our optimal model parameters.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a84c85",
   "metadata": {},
   "source": [
    "### 13. Examine feature importance\n",
    "\n",
    "Let's find out which stats were most important to our model predicting wins. The absolute values of the model coefficients has been saved for you as `importance`. We'll print and plot these scores to see which stat has the highest score.\n",
    "\n",
    "Add code to create a bar plot of the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62690ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get importance\n",
    "optLr.fit(X_train, y_train)\n",
    "importance = abs(optLr.coef_[0])\n",
    "\n",
    "# visualize feature importance\n",
    "sns.barplot(x=importance, y=features.columns)\n",
    "\n",
    "# add labels and titles\n",
    "plt.suptitle('Feature Importance for Logistic Regression')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Stat')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance.round(2)):\n",
    "    print(f'Feature: {features.columns[i]}, Score: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391d736",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary style=\"display:list-item; font-size:16px; color:blue;\"><i>What did we discover in this step? Toggle to check!</i></summary>\n",
    "\n",
    "It looks like the most important stats in our model were turnovers: `TO_offense` and `TO_defense` both had much larger importance scores than the other stats. After that, total yards were the next most influential stats.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79280316",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1c192",
   "metadata": {},
   "source": [
    "### 13. Try your model on new data\n",
    "\n",
    "Congratulations! You've conducted a successful case study on NFL data where the outcome of a game can be predicted using the team's offensive and defensive stats from a given game. \n",
    "\n",
    "Want to see how your model holds up for 2022? Change the `team` variable to your favorite team's name in the code cell below. We've provided the helper function `get_new_data()` that will get that team's data for the given year from the site [Pro Football Reference](https://www.pro-football-reference.com/).\n",
    "\n",
    "We've provided the code for this final step, but we encourage learners who feel confident enough to try the challenge of coding the solution themselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24269af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set team abbreviation (in capitals) and year\n",
    "team = 'Seattle Seahawks'\n",
    "year = 2022\n",
    "\n",
    "# use helper function to pull new data\n",
    "from helper import get_new_data\n",
    "new_data = get_new_data(team=team, year=year)\n",
    "\n",
    "# view head of new data\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f4d52",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary style=\"display:list-item; font-size:16px;\"><i>Need to check the team names? Toggle for code to print a list!</i></summary>\n",
    "\n",
    "Copy and paste this code into a new code cell to get a list of team names.\n",
    "\n",
    "```py\n",
    "list(nfl.team_name.unique())\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94f0188",
   "metadata": {},
   "source": [
    "Before we can run the data in our model and get predictions, we need to standardize the stats using the same `scaler` we used for our original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f56c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select just the game stats\n",
    "new_X = new_data.loc[:,features.columns]\n",
    "\n",
    "# standardize using original data's scaling\n",
    "new_X_sc = scaler.transform(new_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc980b71",
   "metadata": {},
   "source": [
    "Now we can use our model to make predictions and get an accuracy score for how well our model predicted wins with the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3fe65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get new predictions\n",
    "new_preds = optLr.predict(new_X_sc)\n",
    "\n",
    "# get actual results and set type to float\n",
    "new_results = new_data['result'].astype(float)\n",
    "\n",
    "# get accuracy score for new data\n",
    "acc_score = accuracy_score(new_results, new_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eb9eaf",
   "metadata": {},
   "source": [
    "Let's put all this information together in a table and print out our accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f51e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only game data\n",
    "col_names = ['day', 'date', 'result', 'opponent', 'tm_score', 'opp_score']\n",
    "game_data = new_data.loc[:,col_names]\n",
    "# create comparison table\n",
    "comp_table = game_data.assign(predicted = new_preds,\n",
    "                              actual = new_results.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571f96f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print title and table\n",
    "print(f'Predicted Wins vs Actual Wins for {team} in {year}')\n",
    "comp_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057cec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print accuracy\n",
    "print(f'\\nCurrent Accuracy Score: ' + str(round(acc_score*100,1)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a0a60",
   "metadata": {},
   "source": [
    "Our table gives us some context on the game, the opponent, and our prediction. Feel free to go back and change the team name or year (you can look at past years too!)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
